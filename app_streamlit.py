import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier
from sklearn.linear_model import LinearRegression, LogisticRegression
import xgboost as xgb
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import pickle
import re
import os
import plotly.express as px
import plotly.graph_objects as go
from data.data_preprocessing import load_and_preprocess_data, create_price_category
from models.combined_predictor import CombinedHousePricePredictor, visualize_prediction
from models.model_accuracy import evaluate_regression_models, evaluate_classification_models, plot_regression_accuracy, plot_classification_accuracy, plot_class_specific_accuracy

# Set page config
st.set_page_config(
    page_title="D·ª± ƒëo√°n gi√° nh√†",
    page_icon="üè†",
    layout="wide",
    initial_sidebar_state="expanded"
)

@st.cache_data
def load_and_prepare_data():
    # Use the comprehensive preprocessing function
    return load_and_preprocess_data('data/df.csv')

def prepare_sample_data(df):
    # Return a few sample rows from the dataset
    return df.head(10)

def create_preprocessor():
    numeric_features = ['Acreage', 'Num_bedroom', 'Num_WC', 'Num_floor']
    categorical_features = ['District']
    
    # Create preprocessors
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    
    # Create a column transformer
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)
        ])
    
    return preprocessor

def train_model(df, model_type, model_name):
    try:
        X = df[['Acreage', 'Num_bedroom', 'Num_WC', 'Num_floor', 'District']]
        
        if model_type == 'regression':
            y = df['Price']
            # Define models
            models = {
                'Linear Regression': LinearRegression(),
                'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
                'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
                'XGBoost': xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
            }
        else:  # classification
            y = df['Price_Category'].astype(int)
            # Handle potential missing categories
            unique_classes = np.unique(y)
            if len(unique_classes) == 2:  # Binary classification case
                mapping = {unique_classes.min(): 0, unique_classes.max(): 1}
                y = y.map(mapping)
            # Define models
            models = {
                'Logistic Regression': LogisticRegression(max_iter=1000, multi_class='multinomial'),
                'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
                'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
                'XGBoost': xgb.XGBClassifier(objective='multi:softmax', random_state=42)
            }
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Get preprocessor
        preprocessor = create_preprocessor()
        
        # Create pipeline
        pipeline = Pipeline(steps=[
            ('preprocessor', preprocessor),
            ('model', models[model_name])
        ])
        
        # Train model
        pipeline.fit(X_train, y_train)
        
        # Make predictions
        y_pred = pipeline.predict(X_test)
        
        # Evaluate
        if model_type == 'regression':
            metrics = {
                'MSE': mean_squared_error(y_test, y_pred),
                'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),
                'R¬≤': r2_score(y_test, y_pred),
                'MAE': mean_absolute_error(y_test, y_pred)
            }
        else:
            metrics = {
                'Accuracy': accuracy_score(y_test, y_pred),
                'Classification Report': classification_report(y_test, y_pred, output_dict=True)
            }
        
        return pipeline, X_test, y_test, y_pred, metrics
    except Exception as e:
        st.error(f"Error training model: {str(e)}")
        return None, None, None, None, None

def plot_regression_results(X_test, y_test, y_pred):
    # Create DataFrame for plotting
    results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
    
    # Plotly scatter plot
    fig = px.scatter(results_df, x='Actual', y='Predicted',
                    labels={'Actual': 'Actual Price (billion VND)', 'Predicted': 'Predicted Price (billion VND)'},
                    title='Actual vs Predicted Prices',
                    opacity=0.6)
    
    # Add diagonal line (perfect prediction)
    fig.add_trace(go.Scatter(
        x=[results_df['Actual'].min(), results_df['Actual'].max()],
        y=[results_df['Actual'].min(), results_df['Actual'].max()],
        mode='lines',
        name='Perfect prediction',
        line=dict(color='red', dash='dash')
    ))
    
    fig.update_layout(
        width=800,
        height=500,
    )
    
    st.plotly_chart(fig)
    
    # Distribution of errors
    errors = y_test - y_pred
    fig = px.histogram(errors, nbins=50, 
                     labels={'value': 'Error (billion VND)'},
                     title='Distribution of Prediction Errors')
    st.plotly_chart(fig)
    
    # Error vs Actual Price
    error_df = pd.DataFrame({'Actual': y_test, 'Error': errors})
    fig = px.scatter(error_df, x='Actual', y='Error',
                   labels={'Actual': 'Actual Price (billion VND)', 'Error': 'Error (billion VND)'},
                   title='Error vs Actual Price')
    fig.add_hline(y=0, line_dash="dash", line_color="red")
    st.plotly_chart(fig)

def plot_classification_results(y_test, y_pred):
    # Compute union of classes present in the data
    import numpy as np
    classes = np.unique(np.concatenate([y_test, y_pred]))
    # Mapping for all possible classes
    mapping = {
        0: 'Low (<1B)',
        1: 'Medium (1-3B)',
        2: 'High (3-7B)',
        3: 'Very High (>7B)'
    }
    # Use only labels for the classes present
    class_labels = [mapping[c] for c in classes]
    
    # Compute confusion matrix using explicit labels
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(y_test, y_pred, labels=classes)
    
    # Create heatmap with correct labels
    import plotly.express as px
    fig = px.imshow(cm, 
                    x=class_labels, 
                    y=class_labels,
                    labels=dict(x="Predicted Category", y="True Category", color="Count"),
                    text_auto=True,
                    color_continuous_scale='Blues',
                    title='Confusion Matrix')
    st.plotly_chart(fig)
    
    # Classification accuracy by category
    accuracy_by_class = {}
    for i in range(len(class_labels)):
        mask = (y_test == i)
        if mask.sum() > 0:  # Check if we have samples in this class
            accuracy = np.mean(y_pred[mask] == y_test[mask])
            accuracy_by_class[class_labels[i]] = accuracy
    
    # Convert to DataFrame for plotting
    accuracy_df = pd.DataFrame(list(accuracy_by_class.items()), columns=['Category', 'Accuracy'])
    
    fig = px.bar(accuracy_df, x='Category', y='Accuracy',
               labels={'Accuracy': 'Classification Accuracy', 'Category': 'Price Category'},
               title='Classification Accuracy by Price Category')
    fig.update_layout(yaxis_range=[0, 1])
    st.plotly_chart(fig)

def display_feature_importance(model, preprocessor):
    if hasattr(model, 'feature_importances_'):
        # Get feature names
        numeric_features = ['Acreage', 'Num_bedroom', 'Num_WC', 'Num_floor']
        try:
            categorical_features = preprocessor.transformers_[1][2]
            encoder = preprocessor.transformers_[1][1].named_steps['onehot']
            if hasattr(encoder, 'get_feature_names_out'):
                encoded_features = list(encoder.get_feature_names_out(categorical_features))
            else:
                encoded_features = [f"District_{cat}" for cat in encoder.categories_[0]]
            feature_names = numeric_features + encoded_features
        except:
            feature_names = list(range(len(model.feature_importances_)))
        
        # Plot feature importance
        importance_df = pd.DataFrame({
            'Feature': feature_names[:len(model.feature_importances_)],
            'Importance': model.feature_importances_
        }).sort_values('Importance', ascending=False).head(15)  # Top 15 features
        
        fig = px.bar(importance_df, x='Importance', y='Feature', 
                   orientation='h',
                   title='Feature Importance',
                   labels={'Feature': 'Feature', 'Importance': 'Importance Score'})
        st.plotly_chart(fig)

def predict_with_model(pipeline, input_data):
    try:
        # Convert input to DataFrame
        if isinstance(input_data, dict):
            input_df = pd.DataFrame([input_data])
        else:
            input_df = pd.DataFrame(input_data)
        
        # Make prediction
        prediction = pipeline.predict(input_df)
        
        return prediction[0]
    except Exception as e:
        st.error(f"Error making prediction: {str(e)}")
        return None

def main():
    st.title("üè† D·ª± ƒëo√°n gi√° nh√†")
    
    # Load data
    with st.spinner('ƒêang t·∫£i d·ªØ li·ªáu...'):
        df = load_and_prepare_data()
    
    # Create sidebar
    st.sidebar.header("C√†i ƒë·∫∑t")
    
    # Sidebar navigation
    page = st.sidebar.selectbox(
        "Ch·ªçn trang",
        ["üîç Kh√°m ph√° d·ªØ li·ªáu", "üìä Hu·∫•n luy·ªán & ƒê√°nh gi√° m√¥ h√¨nh", "üìà So s√°nh ƒë·ªô ch√≠nh x√°c", "üè° D·ª± ƒëo√°n gi√°"]
    )
    
    if page == "üîç Kh√°m ph√° d·ªØ li·ªáu":
        st.header("Kh√°m ph√° d·ªØ li·ªáu")
        
        # Display sample data
        st.subheader("D·ªØ li·ªáu m·∫´u")
        sample_data = prepare_sample_data(df)
        st.dataframe(sample_data)
        
        # Price distribution
        st.subheader("Ph√¢n b·ªë gi√°")
        col1, col2 = st.columns(2)
        
        with col1:
            fig = px.histogram(df, x='Price', nbins=50,
                             title='Ph√¢n b·ªë gi√°',
                             labels={'Price': 'Gi√° (t·ª∑ VND)', 'count': 'S·ªë l∆∞·ª£ng'})
            fig.update_layout(xaxis_range=[0, 20])  # Limit to 20B for better visualization
            st.plotly_chart(fig)
        
        with col2:
            # Convert Price_Category to string type to avoid issues with narwhals
            category_counts = df['Price_Category'].value_counts().reset_index()
            category_counts.columns = ['Price_Category', 'Count']
            
            # Create mapping for category labels
            category_labels = {
                0: 'Low (<1B)',
                1: 'Medium (1-3B)',
                2: 'High (3-7B)',
                3: 'Very High (>7B)'
            }
            
            # Create a new column with the category labels
            category_counts['Category_Label'] = category_counts['Price_Category'].map(category_labels)
            
            # Create pie chart with the prepared data
            fig = px.pie(
                category_counts, 
                values='Count',
                names='Category_Label', 
                title='Ph√¢n b·ªë c√°c m·ª©c gi√°'
            )
            st.plotly_chart(fig)
        
        # Price vs Acreage
        st.subheader("Gi√° so v·ªõi Di·ªán t√≠ch")
        fig = px.scatter(df.sample(1000) if len(df) > 1000 else df, 
                       x='Acreage', y='Price',
                       color='Price_Category',
                       color_continuous_scale=px.colors.sequential.Viridis,
                       labels={'Acreage': 'Di·ªán t√≠ch (m¬≤)', 'Price': 'Gi√° (t·ª∑ VND)'},
                       title='Gi√° so v·ªõi Di·ªán t√≠ch')
        st.plotly_chart(fig)
        
        # Price by District
        st.subheader("Gi√° theo Qu·∫≠n")
        top_districts = df['District'].value_counts().nlargest(15).index
        district_data = df[df['District'].isin(top_districts)]
        
        fig = px.box(district_data, x='District', y='Price',
                   labels={'District': 'Qu·∫≠n', 'Price': 'Gi√° (t·ª∑ VND)'},
                   title='Ph√¢n b·ªë gi√° theo qu·∫≠n')
        fig.update_layout(xaxis={'categoryorder':'mean descending'})
        st.plotly_chart(fig)
        
        # Correlation matrix
        st.subheader("T∆∞∆°ng quan c√°c ƒë·∫∑c tr∆∞ng")
        corr_data = df[['Price', 'Acreage', 'Num_bedroom', 'Num_WC', 'Price_Category']].corr()
        fig = px.imshow(corr_data,
                      text_auto=True,
                      title='Ma tr·∫≠n t∆∞∆°ng quan',
                      labels=dict(color="H·ªá s·ªë t∆∞∆°ng quan"),
                      color_continuous_scale='RdBu_r',
                      zmin=-1, zmax=1)
        st.plotly_chart(fig)
        
    elif page == "üìä Hu·∫•n luy·ªán & ƒê√°nh gi√° m√¥ h√¨nh":
        st.header("Hu·∫•n luy·ªán & ƒê√°nh gi√° m√¥ h√¨nh")
        
        # Select model type
        model_type = st.sidebar.radio("Ch·ªçn lo·∫°i m√¥ h√¨nh", ["H·ªìi quy", "Ph√¢n lo·∫°i"])
        
        if model_type == "H·ªìi quy":
            st.subheader("M√¥ h√¨nh H·ªìi quy")
            # Select model
            model_name = st.sidebar.selectbox(
                "Ch·ªçn m√¥ h√¨nh H·ªìi quy",
                ["Linear Regression", "Random Forest", "Gradient Boosting", "XGBoost"]
            )
            
            # Train button
            if st.sidebar.button("Hu·∫•n luy·ªán m√¥ h√¨nh"):
                with st.spinner(f'ƒêang hu·∫•n luy·ªán {model_name}...'):
                    pipeline, X_test, y_test, y_pred, metrics = train_model(
                        df, 'regression', model_name
                    )
                
                # Display metrics
                st.subheader("ƒê√°nh gi√° m√¥ h√¨nh")
                metrics_df = pd.DataFrame({
                    'Ch·ªâ s·ªë': list(metrics.keys()),
                    'Gi√° tr·ªã': list(metrics.values())
                })
                st.table(metrics_df.set_index('Ch·ªâ s·ªë'))
                
                # Plot results
                st.subheader("K·∫øt qu·∫£ d·ª± ƒëo√°n")
                plot_regression_results(X_test, y_test, y_pred)
                
                # Display feature importance if available
                st.subheader("T·∫ßm quan tr·ªçng c·ªßa ƒë·∫∑c tr∆∞ng")
                if hasattr(pipeline.named_steps['model'], 'feature_importances_'):
                    display_feature_importance(
                        pipeline.named_steps['model'], 
                        pipeline.named_steps['preprocessor']
                    )
                else:
                    st.info("T·∫ßm quan tr·ªçng c·ªßa ƒë·∫∑c tr∆∞ng kh√¥ng kh·∫£ d·ª•ng ƒë·ªëi v·ªõi m√¥ h√¨nh n√†y.")
        
        else:  # Classification
            st.subheader("M√¥ h√¨nh Ph√¢n lo·∫°i")
            # Select model
            model_name = st.sidebar.selectbox(
                "Ch·ªçn m√¥ h√¨nh Ph√¢n lo·∫°i",
                ["Logistic Regression", "Random Forest", "Gradient Boosting", "XGBoost"]
            )
            
            # Train button
            if st.sidebar.button("Hu·∫•n luy·ªán m√¥ h√¨nh"):
                with st.spinner(f'ƒêang hu·∫•n luy·ªán {model_name}...'):
                    pipeline, X_test, y_test, y_pred, metrics = train_model(
                        df, 'classification', model_name
                    )
                
                # Display metrics
                st.subheader("ƒê√°nh gi√° m√¥ h√¨nh")
                st.write(f"ƒê·ªô ch√≠nh x√°c: {metrics['Accuracy']:.4f}")
                
                # Classification report
                report = metrics['Classification Report']
                class_names = ['Th·∫•p (<1B)', 'Trung b√¨nh (1-3B)', 'Cao (3-7B)', 'R·∫•t cao (>7B)']
                
                # Fix: Handle missing categories in the report
                report_data = {
                    'Precision': [],
                    'Recall': [],
                    'F1-Score': [],
                    'Support': []
                }
                
                available_categories = []
                
                for i in range(4):
                    if str(i) in report:
                        report_data['Precision'].append(report[str(i)]['precision'])
                        report_data['Recall'].append(report[str(i)]['recall'])
                        report_data['F1-Score'].append(report[str(i)]['f1-score'])
                        report_data['Support'].append(report[str(i)]['support'])
                        available_categories.append(class_names[i])
                    else:
                        # If category is missing, add 0 or NaN values
                        report_data['Precision'].append(0)
                        report_data['Recall'].append(0)
                        report_data['F1-Score'].append(0)
                        report_data['Support'].append(0)
                        available_categories.append(class_names[i] + " (kh√¥ng c√≥ m·∫´u)")
                
                report_df = pd.DataFrame(report_data, index=available_categories)
                st.table(report_df)
                
                # Plot results
                st.subheader("K·∫øt qu·∫£ ph√¢n lo·∫°i")
                plot_classification_results(y_test, y_pred)
                
                # Display feature importance if available
                st.subheader("T·∫ßm quan tr·ªçng c·ªßa ƒë·∫∑c tr∆∞ng")
                if hasattr(pipeline.named_steps['model'], 'feature_importances_'):
                    display_feature_importance(
                        pipeline.named_steps['model'], 
                        pipeline.named_steps['preprocessor']
                    )
                else:
                    st.info("T·∫ßm quan tr·ªçng c·ªßa ƒë·∫∑c tr∆∞ng kh√¥ng kh·∫£ d·ª•ng ƒë·ªëi v·ªõi m√¥ h√¨nh n√†y.")
    
    elif page == "üìà So s√°nh ƒë·ªô ch√≠nh x√°c":
        st.header("So s√°nh ƒë·ªô ch√≠nh x√°c c·ªßa c√°c thu·∫≠t to√°n")
        
        with st.spinner("ƒêang t√≠nh to√°n ƒë·ªô ch√≠nh x√°c..."):
            # Cache the evaluation results
            @st.cache_data(ttl=3600)  # Cache for 1 hour
            def get_model_accuracy():
                reg_results = evaluate_regression_models(df)
                cls_results = evaluate_classification_models(df)
                return reg_results, cls_results
            
            reg_results, cls_results = get_model_accuracy()
            
            # Display results in tabs
            tab1, tab2, tab3 = st.tabs(["H·ªìi quy", "Ph√¢n lo·∫°i", "Chi ti·∫øt theo m·ª©c gi√°"])
            
            with tab1:
                st.subheader("ƒê·ªô ch√≠nh x√°c c·ªßa c√°c m√¥ h√¨nh h·ªìi quy")
                
                # Display metrics table - FIX: Changed Viridis to viridis (lowercase)
                st.dataframe(
                    reg_results[['R¬≤', 'RMSE', 'MAE']].style.format({
                        'R¬≤': '{:.4f}',
                        'RMSE': '{:.4f}',
                        'MAE': '{:.4f}'
                    }).background_gradient(cmap='viridis', subset=['R¬≤']).background_gradient(
                        cmap='viridis_r', subset=['RMSE', 'MAE']
                    ),
                    use_container_width=True
                )
                
                # Plot visualization
                st.plotly_chart(plot_regression_accuracy(reg_results))
                
                # Interpretation
                st.subheader("Ph√¢n t√≠ch")
                st.write("""
                - **R¬≤ (H·ªá s·ªë x√°c ƒë·ªãnh)**: C√†ng g·∫ßn 1 c√†ng t·ªët. ƒêo l∆∞·ªùng ph·∫ßn bi·∫øn thi√™n trong d·ªØ li·ªáu ƒë∆∞·ª£c gi·∫£i th√≠ch b·ªüi m√¥ h√¨nh.
                - **RMSE (Root Mean Squared Error)**: C√†ng th·∫•p c√†ng t·ªët. ƒêo l∆∞·ªùng sai s·ªë d·ª± ƒëo√°n trung b√¨nh (t√≠nh b·∫±ng t·ª∑ ƒë·ªìng).
                - **MAE (Mean Absolute Error)**: C√†ng th·∫•p c√†ng t·ªët. ƒêo l∆∞·ªùng sai s·ªë tuy·ªát ƒë·ªëi trung b√¨nh (t√≠nh b·∫±ng t·ª∑ ƒë·ªìng).
                
                D·ª±a v√†o b·∫£ng k·∫øt qu·∫£:
                """)
                
                # Identify best model based on R¬≤
                best_reg_model = reg_results['R¬≤'].idxmax()
                st.write(f"- M√¥ h√¨nh **{best_reg_model}** c√≥ hi·ªáu su·∫•t t·ªët nh·∫•t d·ª±a tr√™n ch·ªâ s·ªë R¬≤ ({reg_results.loc[best_reg_model, 'R¬≤']:.4f}).")
                
                # Identify best model based on RMSE
                best_rmse_model = reg_results['RMSE'].idxmin()
                st.write(f"- M√¥ h√¨nh **{best_rmse_model}** c√≥ sai s·ªë d·ª± ƒëo√°n th·∫•p nh·∫•t v·ªõi RMSE = {reg_results.loc[best_rmse_model, 'RMSE']:.4f} t·ª∑ ƒë·ªìng.")
            
            with tab2:
                st.subheader("ƒê·ªô ch√≠nh x√°c c·ªßa c√°c m√¥ h√¨nh ph√¢n lo·∫°i")
                
                # Display metrics table
                st.dataframe(
                    cls_results[['Accuracy', 'F1 Score', 'Precision', 'Recall']].style.format({
                        'Accuracy': '{:.4f}',
                        'F1 Score': '{:.4f}',
                        'Precision': '{:.4f}',
                        'Recall': '{:.4f}'
                    }).background_gradient(cmap='viridis'),
                    use_container_width=True
                )
                
                # Plot visualization
                st.plotly_chart(plot_classification_accuracy(cls_results))
                
                # Interpretation
                st.subheader("Ph√¢n t√≠ch")
                st.write("""
                - **Accuracy (ƒê·ªô ch√≠nh x√°c)**: T·ª∑ l·ªá d·ª± ƒëo√°n ƒë√∫ng tr√™n t·ªïng s·ªë d·ª± ƒëo√°n.
                - **F1 Score**: Trung b√¨nh ƒëi·ªÅu h√≤a c·ªßa precision v√† recall, gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ m·∫•t c√¢n b·∫±ng d·ªØ li·ªáu.
                - **Precision (ƒê·ªô ch√≠nh x√°c)**: Trong c√°c tr∆∞·ªùng h·ª£p ƒë∆∞·ª£c d·ª± ƒëo√°n l√† d∆∞∆°ng t√≠nh, bao nhi√™u th·ª±c s·ª± ƒë√∫ng.
                - **Recall (ƒê·ªô nh·∫°y)**: Trong s·ªë c√°c tr∆∞·ªùng h·ª£p th·ª±c s·ª± d∆∞∆°ng t√≠nh, bao nhi√™u ƒë∆∞·ª£c nh·∫≠n di·ªán ƒë√∫ng.
                
                D·ª±a v√†o b·∫£ng k·∫øt qu·∫£:
                """)
                
                # Identify best model based on Accuracy
                best_cls_model = cls_results['Accuracy'].idxmax()
                st.write(f"- M√¥ h√¨nh **{best_cls_model}** c√≥ ƒë·ªô ch√≠nh x√°c cao nh·∫•t ({cls_results.loc[best_cls_model, 'Accuracy']:.4f}).")
                
                # Identify best model based on F1
                best_f1_model = cls_results['F1 Score'].idxmax()
                st.write(f"- M√¥ h√¨nh **{best_f1_model}** c√≥ F1 Score cao nh·∫•t ({cls_results.loc[best_f1_model, 'F1 Score']:.4f}).")
            
            with tab3:
                st.subheader("ƒê·ªô ch√≠nh x√°c ph√¢n lo·∫°i theo m·ª©c gi√°")
                
                # Compute and plot class-specific accuracy
                with st.spinner("ƒêang t√≠nh to√°n ƒë·ªô ch√≠nh x√°c theo m·ª©c gi√°..."):
                    fig = plot_class_specific_accuracy(df)
                    st.plotly_chart(fig)
                
                st.write("""
                Bi·ªÉu ƒë·ªì tr√™n cho th·∫•y ƒë·ªô ch√≠nh x√°c c·ªßa t·ª´ng m√¥ h√¨nh ƒë·ªëi v·ªõi c√°c m·ª©c gi√° kh√°c nhau.
                M·ªôt s·ªë m√¥ h√¨nh c√≥ th·ªÉ ho·∫°t ƒë·ªông t·ªët v·ªõi m·ª©c gi√° th·∫•p nh∆∞ng l·∫°i k√©m ch√≠nh x√°c v·ªõi m·ª©c gi√° cao, ho·∫∑c ng∆∞·ª£c l·∫°i.
                """)
                
                st.info("""
                **Ghi ch√∫**: ƒê·ªô ch√≠nh x√°c c√≥ th·ªÉ th·∫•p ·ªü m·ªôt s·ªë nh√≥m do:
                - S·ªë l∆∞·ª£ng m·∫´u kh√¥ng ƒë·ªìng ƒë·ªÅu gi·ªØa c√°c m·ª©c gi√°
                - ƒê·∫∑c tr∆∞ng c·ªßa nh√† ·ªü m·ª©c gi√° cao th∆∞·ªùng ƒëa d·∫°ng h∆°n v√† kh√≥ d·ª± ƒëo√°n h∆°n
                - M·ªôt s·ªë nh√≥m c√≥ th·ªÉ c√≥ qu√° √≠t d·ªØ li·ªáu ƒë·ªÉ m√¥ h√¨nh h·ªçc hi·ªáu qu·∫£
                """)
    
    else:  # Price Prediction
        st.header("D·ª± ƒëo√°n gi√° nh√†")
        
        # Use combined predictor or separate models
        prediction_approach = st.sidebar.radio(
            "Ph∆∞∆°ng ph√°p d·ª± ƒëo√°n",
            ["T√≠ch h·ª£p (H·ªìi quy + Ph√¢n lo·∫°i)", "Ch·ªâ H·ªìi quy", "Ch·ªâ Ph√¢n lo·∫°i"]
        )
        
        # Add model selection based on approach - MOVED OUTSIDE THE BUTTON CLICK HANDLER
        selected_model = None
        if prediction_approach == "Ch·ªâ H·ªìi quy":
            # Import regression model components
            from features.feature_engineering import create_preprocessor
            from models.regression_models import get_regression_models
            
            # Let user select regression model
            reg_models = get_regression_models()
            selected_model = st.sidebar.selectbox(
                "Ch·ªçn m√¥ h√¨nh h·ªìi quy",
                list(reg_models.keys()),
                index=1  # Default to Random Forest
            )
        
        elif prediction_approach == "Ch·ªâ Ph√¢n lo·∫°i":
            # Import classification model components
            from features.feature_engineering import create_preprocessor
            from models.classification_models import get_classification_models
            
            # Let user select classification model
            cls_models = get_classification_models()
            selected_model = st.sidebar.selectbox(
                "Ch·ªçn m√¥ h√¨nh ph√¢n lo·∫°i",
                list(cls_models.keys()),
                index=1  # Default to Random Forest
            )
        
        # Input form for house features
        st.subheader("Nh·∫≠p th√¥ng tin nh√†")
        
        col1, col2 = st.columns(2)
        
        with col1:
            area = st.number_input("Di·ªán t√≠ch (m¬≤)", min_value=10.0, max_value=1000.0, value=80.0)
            num_bedrooms = st.number_input("S·ªë ph√≤ng ng·ªß", min_value=0, max_value=20, value=2)
            num_floors = st.number_input("S·ªë t·∫ßng", min_value=1, max_value=50, value=1)

        with col2:
            num_bathrooms = st.number_input("S·ªë ph√≤ng t·∫Øm", min_value=0, max_value=20, value=2)
            district_list = sorted(df['District'].unique().tolist())
            district = st.selectbox("Qu·∫≠n", district_list, index=district_list.index('Qu·∫≠n 7') if 'Qu·∫≠n 7' in district_list else 0)
        
        # Create input data
        input_data = {
            'Acreage': area,
            'Num_bedroom': num_bedrooms,
            'Num_WC': num_bathrooms,
            'Num_floor': num_floors,
            'District': district
        }
        
        # Predict button
        if st.button("D·ª± ƒëo√°n gi√°"):
            if prediction_approach == "T√≠ch h·ª£p (H·ªìi quy + Ph√¢n lo·∫°i)":
                with st.spinner('ƒêang hu·∫•n luy·ªán m√¥ h√¨nh t√≠ch h·ª£p...'):
                    predictor = CombinedHousePricePredictor()
                    predictor.train(df)
                    
                    prediction = predictor.predict(input_data)
                    
                    # Display prediction
                    st.subheader("K·∫øt qu·∫£ d·ª± ƒëo√°n gi√°")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.metric(
                            "Gi√° d·ª± ƒëo√°n", 
                            f"{prediction['predicted_price']:.2f} t·ª∑ VND",
                            delta=None
                        )
                        st.write(f"**Kho·∫£ng tin c·∫≠y 90%:** {prediction['price_confidence_interval'][0]:.2f} - {prediction['price_confidence_interval'][1]:.2f} t·ª∑ VND")
                    
                    with col2:
                        st.metric(
                            "M·ª©c gi√° d·ª± ƒëo√°n",
                            prediction['predicted_category_name']
                        )
                        
                        # Display category probabilities
                        st.write("**X√°c su·∫•t c·ªßa t·ª´ng m·ª©c gi√°:**")
                        for category, prob in prediction['category_probabilities'].items():
                            st.write(f"- {category}: {prob:.2f}")
                    
                    # Visualization
                    st.subheader("Bi·ªÉu ƒë·ªì d·ª± ƒëo√°n")
                    
                    # Create plots
                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
                    
                    # Plot 1: Price prediction with confidence interval
                    price_pred = prediction['predicted_price']
                    lower_bound, upper_bound = prediction['price_confidence_interval']
                    
                    ax1.barh(['Gi√°'], [price_pred], color='skyblue', alpha=0.7, height=0.5)
                    ax1.errorbar([price_pred], [0], xerr=[[price_pred-lower_bound], [upper_bound-price_pred]], 
                                fmt='o', color='black', capsize=10)
                    ax1.set_xlim(0, upper_bound * 1.1)
                    ax1.set_title('Gi√° d·ª± ƒëo√°n v·ªõi kho·∫£ng tin c·∫≠y 90%')
                    ax1.set_xlabel('Gi√° (t·ª∑ VND)')
                    ax1.set_yticks([])
                    
                    # Price labels
                    ax1.text(price_pred, 0, f'{price_pred:.2f}B', 
                            horizontalalignment='center', verticalalignment='bottom', fontweight='bold')
                    ax1.text(lower_bound, 0, f'{lower_bound:.2f}B', 
                            horizontalalignment='center', verticalalignment='top')
                    
                    plt.tight_layout()
                    st.pyplot(fig)
                    
                    # Similar properties
                    st.subheader("C√°c b·∫•t ƒë·ªông s·∫£n t∆∞∆°ng t·ª±")
                    
                    # Filter properties in the same price range and district
                    similar_props = df[
                        (df['District'] == district) & 
                        (df['Price'] >= prediction['price_confidence_interval'][0]) &
                        (df['Price'] <= prediction['price_confidence_interval'][1])
                    ].head(5)
                    
                    if len(similar_props) > 0:
                        st.dataframe(similar_props[['Acreage', 'Num_bedroom', 'Num_WC', 'Num_floor', 'District', 'Price']])
                    else:
                        st.write("Kh√¥ng t√¨m th·∫•y b·∫•t ƒë·ªông s·∫£n t∆∞∆°ng t·ª±.")
                    
            elif prediction_approach == "Ch·ªâ H·ªìi quy":
                with st.spinner('ƒêang hu·∫•n luy·ªán m√¥ h√¨nh h·ªìi quy...'):
                    # Create preprocessor
                    preprocessor = create_preprocessor()
                    
                    # Use the already selected model (we don't need to show the selection again)
                    # Create and train pipeline
                    from sklearn.pipeline import Pipeline
                    pipeline = Pipeline(steps=[
                        ('preprocessor', preprocessor),
                        ('model', reg_models[selected_model])
                    ])
                    
                    # Convert input_data to DataFrame for prediction
                    input_df = pd.DataFrame([input_data])
                    
                    # Fit model on data
                    X = df[['Acreage', 'Num_bedroom', 'Num_WC', 'Num_floor', 'District']]
                    y = df['Price']
                    pipeline.fit(X, y)
                    
                    # Make prediction
                    predicted_price = pipeline.predict(input_df)[0]
                    
                    # Display results
                    st.subheader("K·∫øt qu·∫£ d·ª± ƒëo√°n gi√°")
                    
                    st.metric(
                        "Gi√° d·ª± ƒëo√°n", 
                        f"{predicted_price:.2f} t·ª∑ VND",
                        delta=None
                    )
                    
                    # Simple confidence interval (¬±15%)
                    lower_bound = predicted_price * 0.85
                    upper_bound = predicted_price * 1.15
                    
                    st.write(f"**Kho·∫£ng tin c·∫≠y ƒë∆°n gi·∫£n:** {lower_bound:.2f} - {upper_bound:.2f} t·ª∑ VND")
                    
                    # Create a simple visualization
                    fig, ax = plt.subplots(figsize=(10, 4))
                    ax.barh(['Gi√°'], [predicted_price], color='skyblue', alpha=0.7, height=0.5)
                    ax.errorbar([predicted_price], [0], xerr=[[predicted_price-lower_bound], [upper_bound-predicted_price]], 
                               fmt='o', color='black', capsize=10)
                    ax.set_xlim(0, upper_bound * 1.1)
                    ax.set_title(f'Gi√° d·ª± ƒëo√°n - M√¥ h√¨nh {selected_model}')
                    ax.set_xlabel('Gi√° (t·ª∑ VND)')
                    ax.set_yticks([])
                    ax.text(predicted_price, 0, f'{predicted_price:.2f}B', 
                           horizontalalignment='center', verticalalignment='bottom', fontweight='bold')
                    
                    plt.tight_layout()
                    st.pyplot(fig)
                    
                    # Add similar properties section
                    st.subheader("C√°c b·∫•t ƒë·ªông s·∫£n t∆∞∆°ng t·ª±")
                    
                    # Filter properties in the same price range and district
                    similar_props = df[
                        (df['District'] == district) & 
                        (df['Price'] >= lower_bound) &
                        (df['Price'] <= upper_bound)
                    ].head(5)
                    
                    if len(similar_props) > 0:
                        st.dataframe(similar_props[['Acreage', 'Num_bedroom', 'Num_WC', 'Num_floor', 'District', 'Price']])
                        
                        # Add a visualization comparing the prediction with similar properties
                        fig, ax = plt.subplots(figsize=(10, 5))
                        
                        # Plot prices of similar properties
                        ax.bar(range(len(similar_props)), similar_props['Price'], color='lightgray', 
                               alpha=0.7, label='Nh√† t∆∞∆°ng t·ª±')
                        
                        # Plot prediction as a horizontal line
                        ax.axhline(y=predicted_price, color='red', linestyle='-', 
                                  label=f'D·ª± ƒëo√°n: {predicted_price:.2f}B')
                        
                        # Add price labels above each bar
                        for i, price in enumerate(similar_props['Price']):
                            ax.text(i, price + 0.1, f'{price:.2f}B', ha='center')
                        
                        ax.set_ylabel('Gi√° (t·ª∑ VND)')
                        ax.set_title('So s√°nh v·ªõi c√°c b·∫•t ƒë·ªông s·∫£n t∆∞∆°ng t·ª±')
                        ax.set_xticks(range(len(similar_props)))
                        ax.set_xticklabels([f'{a}m¬≤, {b}PN, {c}WC' for a, b, c in 
                                          zip(similar_props['Acreage'], similar_props['Num_bedroom'], similar_props['Num_WC'])],
                                         rotation=45, ha='right')
                        ax.legend()
                        plt.tight_layout()
                        st.pyplot(fig)
                    else:
                        st.write("Kh√¥ng t√¨m th·∫•y b·∫•t ƒë·ªông s·∫£n t∆∞∆°ng t·ª±.")
            
            elif prediction_approach == "Ch·ªâ Ph√¢n lo·∫°i":
                with st.spinner('ƒêang hu·∫•n luy·ªán m√¥ h√¨nh ph√¢n lo·∫°i...'):
                    # Create preprocessor
                    preprocessor = create_preprocessor()
                    
                    # Use the already selected model (we don't need to show the selection again)
                    # Create and train pipeline
                    from sklearn.pipeline import Pipeline
                    pipeline = Pipeline(steps=[
                        ('preprocessor', preprocessor),
                        ('model', cls_models[selected_model])
                    ])
                    
                    # Convert input_data to DataFrame for prediction
                    input_df = pd.DataFrame([input_data])
                    
                    # Define price categories for training
                    conditions = [
                        (df['Price'] < 1),
                        (df['Price'] >= 1) & (df['Price'] < 3),
                        (df['Price'] >= 3) & (df['Price'] < 7),
                        (df['Price'] >= 7)
                    ]
                    choices = [0, 1, 2, 3]
                    df['Price_Category'] = np.select(conditions, choices, default=np.nan)
                    
                    # Fit model on data
                    X = df[['Acreage', 'Num_bedroom', 'Num_WC', 'Num_floor', 'District']]
                    y = df['Price_Category'].astype(int)
                    pipeline.fit(X, y)
                    
                    # Make prediction
                    predicted_class = pipeline.predict(input_df)[0]
                    
                    # Get probabilities
                    price_categories = {
                        0: 'Low price (< 1B)',
                        1: 'Medium price (1-3B)',
                        2: 'High price (3-7B)',
                        3: 'Very high price (> 7B)'
                    }
                    
                    try:
                        proba = pipeline.predict_proba(input_df)[0]
                        category_probs = {price_categories[i]: prob for i, prob in enumerate(proba)}
                    except:
                        category_probs = {price_categories[predicted_class]: 1.0}
                    
                    # Display results
                    st.subheader("K·∫øt qu·∫£ d·ª± ƒëo√°n m·ª©c gi√°")
                    
                    st.metric(
                        "M·ª©c gi√° d·ª± ƒëo√°n", 
                        price_categories[predicted_class],
                        delta=None
                    )
                    
                    # Display category probabilities
                    st.write("**X√°c su·∫•t c·ªßa t·ª´ng m·ª©c gi√°:**")
                    for category, prob in category_probs.items():
                        st.write(f"- {category}: {prob:.2f}")
                    
                    # Create a visualization of the probability distribution
                    fig, ax = plt.subplots(figsize=(10, 6))
                    
                    categories = list(category_probs.keys())
                    probabilities = list(category_probs.values())
                    
                    bars = ax.bar(categories, probabilities, color=['lightgreen', 'skyblue', 'orange', 'salmon'])
                    
                    # Highlight the predicted class
                    bars[predicted_class].set_color('darkblue')
                    
                    ax.set_ylim(0, 1.0)
                    ax.set_ylabel('X√°c su·∫•t')
                    ax.set_title(f'Ph√¢n b·ªë x√°c su·∫•t c√°c m·ª©c gi√° - M√¥ h√¨nh {selected_model}')
                    
                    # Add probability labels on top of bars
                    for bar in bars:
                        height = bar.get_height()
                        ax.annotate(f'{height:.2f}',
                                    xy=(bar.get_x() + bar.get_width() / 2, height),
                                    xytext=(0, 3),  # 3 points vertical offset
                                    textcoords="offset points",
                                    ha='center', va='bottom')
                    
                    plt.xticks(rotation=45, ha='right')
                    plt.tight_layout()
                    st.pyplot(fig)
                    
                    # Add similar properties section
                    st.subheader("C√°c b·∫•t ƒë·ªông s·∫£n t∆∞∆°ng t·ª±")
                    
                    # Map predicted class to price range
                    price_ranges = {
                        0: (0, 1),       # Low: < 1B
                        1: (1, 3),       # Medium: 1-3B
                        2: (3, 7),       # High: 3-7B
                        3: (7, float('inf'))  # Very High: > 7B
                    }
                    
                    min_price, max_price = price_ranges[predicted_class]
                    
                    # Filter properties in the same category and district
                    similar_props = df[
                        (df['District'] == district) & 
                        (df['Price'] >= min_price) &
                        (df['Price'] <= max_price)
                    ].head(5)
                    
                    if len(similar_props) > 0:
                        st.dataframe(similar_props[['Acreage', 'Num_bedroom', 'Num_WC', 'Num_floor', 'District', 'Price']])
                        
                        # Add a visualization of similar properties in this category
                        fig, ax = plt.subplots(figsize=(10, 5))
                        
                        # Plot scatter of similar properties by price vs size
                        scatter = ax.scatter(similar_props['Acreage'], similar_props['Price'], 
                                           s=similar_props['Num_bedroom']*30, alpha=0.7,
                                           c=similar_props['Num_WC'], cmap='viridis')
                        
                        # Highlight the user's input
                        ax.scatter([area], [(min_price + max_price)/2], marker='*', color='red', s=200,
                                 label='D·ª± ƒëo√°n c·ªßa b·∫°n')
                        
                        # Add a legend for bedroom size
                        handles, labels = scatter.legend_elements(prop="sizes", alpha=0.6, 
                                                                num=sorted(similar_props['Num_bedroom'].unique()))
                        bedroom_legend = ax.legend(handles, 
                                                 [f'{int(size/30)} ph√≤ng ng·ªß' for size in sorted(similar_props['Num_bedroom'].unique()*30)],
                                                 loc="upper left", title="S·ªë ph√≤ng ng·ªß")
                        ax.add_artist(bedroom_legend)
                        
                        # Add a colorbar for bathroom count
                        cbar = plt.colorbar(scatter)
                        cbar.set_label('S·ªë ph√≤ng t·∫Øm')
                        
                        ax.set_title(f'C√°c b·∫•t ƒë·ªông s·∫£n trong m·ª©c gi√°: {price_categories[predicted_class]}')
                        ax.set_xlabel('Di·ªán t√≠ch (m¬≤)')
                        ax.set_ylabel('Gi√° (t·ª∑ VND)')
                        plt.tight_layout()
                        st.pyplot(fig)
                    else:
                        st.write("Kh√¥ng t√¨m th·∫•y b·∫•t ƒë·ªông s·∫£n t∆∞∆°ng t·ª±.")

    # Footer
    st.sidebar.markdown("---")
    st.sidebar.info(
        """
        D·ª± √°n D·ª± ƒëo√°n Gi√° Nh√†
        
        K·∫øt h·ª£p ph∆∞∆°ng ph√°p h·ªìi quy v√† ph√¢n lo·∫°i ƒë·ªÉ d·ª± ƒëo√°n gi√° nh√†.
        """
    )

if __name__ == "__main__":
    main()
